{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install snntorch","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:22:45.513297Z","iopub.execute_input":"2024-01-04T08:22:45.513587Z","iopub.status.idle":"2024-01-04T08:23:01.233869Z","shell.execute_reply.started":"2024-01-04T08:22:45.513559Z","shell.execute_reply":"2024-01-04T08:23:01.232798Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting snntorch\n  Obtaining dependency information for snntorch from https://files.pythonhosted.org/packages/4b/55/95ee9e0e26cf74a464603ef7ab84be186133bfb95ac0c5ae9d1eb408b69b/snntorch-0.7.0-py2.py3-none-any.whl.metadata\n  Downloading snntorch-0.7.0-py2.py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: torch>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from snntorch) (2.0.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from snntorch) (2.0.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from snntorch) (3.7.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from snntorch) (1.24.3)\nCollecting nir (from snntorch)\n  Obtaining dependency information for nir from https://files.pythonhosted.org/packages/a8/e1/60b9014266c26d170b2f1bc7fe1b7b6ad823ad8cb302104ca306685311ac/nir-1.0.1-py3-none-any.whl.metadata\n  Downloading nir-1.0.1-py3-none-any.whl.metadata (5.8 kB)\nCollecting nirtorch (from snntorch)\n  Obtaining dependency information for nirtorch from https://files.pythonhosted.org/packages/cd/74/92cc684fd83636b072318693676877af0d80c4e136067237f147f9a18d6f/nirtorch-1.0-py3-none-any.whl.metadata\n  Downloading nirtorch-1.0-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->snntorch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->snntorch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->snntorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->snntorch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->snntorch) (3.1.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->snntorch) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->snntorch) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->snntorch) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->snntorch) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->snntorch) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->snntorch) (10.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->snntorch) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->snntorch) (2.8.2)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from nir->snntorch) (3.9.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->snntorch) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->snntorch) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\nDownloading snntorch-0.7.0-py2.py3-none-any.whl (108 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hDownloading nir-1.0.1-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nirtorch-1.0-py3-none-any.whl (13 kB)\nInstalling collected packages: nir, nirtorch, snntorch\nSuccessfully installed nir-1.0.1 nirtorch-1.0 snntorch-0.7.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SNN Autoencoder (SAE)","metadata":{}},{"cell_type":"code","source":"import os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import utils as utls\n\nimport snntorch as snn\nfrom snntorch import utils\nfrom snntorch import surrogate\n\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:23:01.235762Z","iopub.execute_input":"2024-01-04T08:23:01.236103Z","iopub.status.idle":"2024-01-04T08:23:06.960759Z","shell.execute_reply.started":"2024-01-04T08:23:01.236074Z","shell.execute_reply":"2024-01-04T08:23:06.959930Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class SAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #Encoder\n        self.encoder = nn.Sequential(nn.Conv2d(1, 32, 3,padding = 1,stride=2),\n                          nn.BatchNorm2d(32),\n                          snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n                          nn.Conv2d(32, 64, 3,padding = 1,stride=2),\n                          nn.BatchNorm2d(64),\n                          snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n                          nn.Conv2d(64, 128, 3,padding = 1,stride=2),\n                          nn.BatchNorm2d(128),\n                          snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n                          nn.Flatten(start_dim = 1, end_dim = 3),\n                          nn.Linear(2048, latent_dim), #this needs to be the final layer output size (channels * pixels * pixels)\n                          snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n                          )\n       # From latent back to tensor for convolution\n        self.linearNet= nn.Sequential(nn.Linear(latent_dim,128*4*4),\n                               snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))        #Decoder\n\n        self.decoder = nn.Sequential(nn.Unflatten(1,(128,4,4)),\n                          snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n                          nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1),\n                          nn.BatchNorm2d(64),\n                          snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n                          nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1),\n                          nn.BatchNorm2d(32),\n                          snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n                          nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1),\n                          snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #make large so membrane can be trained\n                          )\n\n    def forward(self, x): #Dimensions: [Batch,Channels,Width,Length]\n        utils.reset(self.encoder) #need to reset the hidden states of LIF\n        utils.reset(self.decoder)\n        utils.reset(self.linearNet)\n\n        #encode\n        spk_mem=[];spk_rec=[];encoded_x=[]\n        for step in range(num_steps): #for t in time\n            spk_x,mem_x=self.encode(x) #Output spike trains and neuron membrane states\n            spk_rec.append(spk_x)\n            spk_mem.append(mem_x)\n        spk_rec=torch.stack(spk_rec,dim=2)\n        spk_mem=torch.stack(spk_mem,dim=2) #Dimensions:[Batch,Channels,Width,Length, Time]\n\n        #decode\n        spk_mem2=[];spk_rec2=[];decoded_x=[]\n        for step in range(num_steps): #for t in time\n            x_recon,x_mem_recon=self.decode(spk_rec[...,step])\n            spk_rec2.append(x_recon)\n            spk_mem2.append(x_mem_recon)\n        spk_rec2=torch.stack(spk_rec2,dim=4)\n        spk_mem2=torch.stack(spk_mem2,dim=4)#Dimensions:[Batch,Channels,Width,Length, Time]\n        out = spk_mem2[:,:,:,:,-1] #return the membrane potential of the output neuron at t = -1 (last t)\n        return out #Dimensions:[Batch,Channels,Width,Length]\n\n    def encode(self,x):\n        spk_latent_x,mem_latent_x=self.encoder(x)\n        return spk_latent_x,mem_latent_x\n\n    def decode(self,x):\n        spk_x,mem_x = self.linearNet(x) #convert latent dimension back to total size of features in encoder final layer\n        spk_x2,mem_x2=self.decoder(spk_x)\n        return spk_x2,mem_x2","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:23:06.961842Z","iopub.execute_input":"2024-01-04T08:23:06.962221Z","iopub.status.idle":"2024-01-04T08:23:06.980767Z","shell.execute_reply.started":"2024-01-04T08:23:06.962196Z","shell.execute_reply":"2024-01-04T08:23:06.979861Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef train(network, trainloader, opti, epoch, device):\n    network = network.train()\n    train_loss_hist = []\n    total_loss = 0\n\n    # Ensure the training directory exists\n    training_dir = '/kaggle/working/training/'\n    os.makedirs(training_dir, exist_ok=True)\n\n    for batch_idx, (real_img, labels) in enumerate(trainloader):\n        opti.zero_grad()\n        real_img = real_img.to(device)\n\n        # Pass data into network, and return reconstructed image from Membrane Potential at t = -1\n        x_recon = network(real_img)  # Dimensions passed in: [Batch_size, Input_size, Image_Width, Image_Length]\n\n        # Calculate loss\n        loss_val = F.mse_loss(x_recon, real_img)\n\n        # Backpropagate and update weights\n        loss_val.backward()\n        opti.step()\n        total_loss += loss_val.item()\n\n        # Add the current batch loss to the history\n        # train_loss_hist.append(loss_val.item())\n\n        # Print the loss for the current batch\n      \n\n        '''\n        # Save reconstructed images at the end of the epoch\n        if batch_idx == len(trainloader) - 1:\n            # Normalize images to [0, 1] for proper saving\n            real_img_norm = (real_img - real_img.min()) / (real_img.max() - real_img.min())\n            x_recon_norm = (x_recon - x_recon.min()) / (x_recon.max() - x_recon.min())\n\n            # Save the images using torchvision's save_image function\n            utls.save_image(real_img_norm, os.path.join(training_dir, f'epoch{epoch}_finalbatch_inputs.png'))\n            utls.save_image(x_recon_norm, os.path.join(training_dir, f'epoch{epoch}_finalbatch_recon.png'))\n'''\n    # Return the average loss for this epoch\n        # train_loss += loss_val.item()*images.size(0)\n            \n    # print avg training statistics \n    average_loss = total_loss/len(train_loader)\n    train_loss_hist.append(average_loss)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, average_loss))\n    # return avg_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:23:06.983152Z","iopub.execute_input":"2024-01-04T08:23:06.983621Z","iopub.status.idle":"2024-01-04T08:23:06.999173Z","shell.execute_reply.started":"2024-01-04T08:23:06.983583Z","shell.execute_reply":"2024-01-04T08:23:06.998309Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Parameters and Run training and testing\nbatch_size = 250\ninput_size = 32 #size of input to first convolutional layer\n\n#setup GPU\ndtype = torch.float\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# Load MNIST\ntrain_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((input_size, input_size)),transforms.Normalize((0,), (1,))]), download=True)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((input_size, input_size)),transforms.Normalize((0,), (1,))]), download=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:23:07.000317Z","iopub.execute_input":"2024-01-04T08:23:07.000665Z","iopub.status.idle":"2024-01-04T08:23:08.294005Z","shell.execute_reply.started":"2024-01-04T08:23:07.000631Z","shell.execute_reply":"2024-01-04T08:23:08.292980Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 102293377.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 85306826.64it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1648877/1648877 [00:00<00:00, 24517134.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 7842951.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# SNN parameters\nspike_grad = surrogate.atan(alpha=2.0)# alternate surrogate gradient: fast_sigmoid(slope=25) \nbeta = 0.5 #decay rate of neurons \nnum_steps=5 #time \nlatent_dim = 32 #dimension of latent layer (how compressed we want the information)\nthresh=1#spiking threshold (lower = more spikes are let through)\nepochs=30 #number of epochs\nmax_epoch=epochs\n\n#Define Network and optimizer\nnet=SAE()\nnet = net.to(device)\n\noptimizer = torch.optim.AdamW(net.parameters(), \n                            lr=0.0001,\n                            betas=(0.9, 0.999), \n                            weight_decay=0.001)\n\n#Run training and testing        \nfor e in range(epochs): \n    train(net, train_loader, optimizer, e,device)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T04:20:06.882189Z","iopub.execute_input":"2024-01-04T04:20:06.882589Z","iopub.status.idle":"2024-01-04T04:37:57.462986Z","shell.execute_reply.started":"2024-01-04T04:20:06.882556Z","shell.execute_reply":"2024-01-04T04:37:57.461891Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0 \tTraining Loss: 0.300347\nEpoch: 1 \tTraining Loss: 0.134863\nEpoch: 2 \tTraining Loss: 0.112710\nEpoch: 3 \tTraining Loss: 0.101490\nEpoch: 4 \tTraining Loss: 0.091986\nEpoch: 5 \tTraining Loss: 0.082478\nEpoch: 6 \tTraining Loss: 0.074407\nEpoch: 7 \tTraining Loss: 0.067139\nEpoch: 8 \tTraining Loss: 0.060876\nEpoch: 9 \tTraining Loss: 0.055533\nEpoch: 10 \tTraining Loss: 0.050633\nEpoch: 11 \tTraining Loss: 0.046136\nEpoch: 12 \tTraining Loss: 0.042324\nEpoch: 13 \tTraining Loss: 0.038793\nEpoch: 14 \tTraining Loss: 0.035590\nEpoch: 15 \tTraining Loss: 0.032958\nEpoch: 16 \tTraining Loss: 0.030676\nEpoch: 17 \tTraining Loss: 0.028745\nEpoch: 18 \tTraining Loss: 0.027008\nEpoch: 19 \tTraining Loss: 0.025682\nEpoch: 20 \tTraining Loss: 0.024585\nEpoch: 21 \tTraining Loss: 0.023593\nEpoch: 22 \tTraining Loss: 0.022728\nEpoch: 23 \tTraining Loss: 0.022042\nEpoch: 24 \tTraining Loss: 0.021403\nEpoch: 25 \tTraining Loss: 0.020846\nEpoch: 26 \tTraining Loss: 0.020379\nEpoch: 27 \tTraining Loss: 0.019951\nEpoch: 28 \tTraining Loss: 0.019569\nEpoch: 29 \tTraining Loss: 0.019234\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the network to evaluation mode\nnetwork = net.eval()\n\ntest_loss_hist = []\ntotal_loss = 0\n\n# Disable gradient calculations\nwith torch.no_grad():\n    for batch_idx, (real_img, labels) in enumerate(test_loader):\n        real_img = real_img.to(device)\n\n        # Pass data into network\n        x_recon = net(real_img)  # Dimensions passed in: [Batch_size, Input_size, Image_Width, Image_Length]\n\n        # Calculate loss\n        loss_val = F.mse_loss(x_recon, real_img)\n        total_loss += loss_val.item()\n\n        # Optional: Code to display/save images or other test-time operations\n\n# Calculate average loss for the test data\naverage_loss = total_loss / len(test_loader)\ntest_loss_hist.append(average_loss)\n\nprint('Test Loss: {:.6f}'.format(average_loss))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:24:43.148325Z","iopub.execute_input":"2024-01-04T08:24:43.149151Z","iopub.status.idle":"2024-01-04T08:24:56.834025Z","shell.execute_reply.started":"2024-01-04T08:24:43.149116Z","shell.execute_reply":"2024-01-04T08:24:56.832996Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.092623\n","output_type":"stream"}]}]}